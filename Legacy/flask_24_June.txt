import os
import shutil
import sys
import threading
import uuid
import time
import subprocess
import logging
from flask import Flask, request, jsonify
from logging.handlers import RotatingFileHandler
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# ==== LOGGING SETUP ====
LOG_PATH = os.path.join(os.path.dirname(__file__), "pipeline.log")
handler = RotatingFileHandler(LOG_PATH, maxBytes=5_000_000, backupCount=3, encoding="utf-8")
formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(threadName)s | %(message)s')
handler.setFormatter(formatter)
logging.basicConfig(handlers=[handler], level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Pipeline backend starting up.")

# ==== PATHS ====
SCRIPTS_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'scripts'))

OUTPUT_FOLDERS = [
    os.path.join(SCRIPTS_PATH, 'scraping', 'Justlit-dir'),
    os.path.join(SCRIPTS_PATH, 'ingest', 'output2'),
    os.path.join(SCRIPTS_PATH, 'rag2', 'chunked'),
    os.path.join(SCRIPTS_PATH, 'rag2', 'vector_db'),
    os.path.join(SCRIPTS_PATH, 'rag2', 'query_results')
]

ACTIVE_PIPELINES = {}

type_order = [
    "scrape", "parse_pdf", "parse_csv", "parse_excel", "parse_json", "parse_docx", "parse_text",
    "chunk_texts", "embed_store", "rag_query"
]

def cleanup_outputs():
    logger.info("Cleaning up output folders before pipeline run.")
    for folder in OUTPUT_FOLDERS:
        try:
            if os.path.exists(folder):
                logger.info(f"Deleting folder: {folder}")
                shutil.rmtree(folder)
            os.makedirs(folder, exist_ok=True)
        except Exception as e:
            logger.error(f"Error cleaning/creating folder {folder}: {e}")

def run_script(script_path, params):
    env = os.environ.copy()
    for key, value in params.items():
        env[str(key)] = str(value)
    logger.info(f"Running script: {script_path} with params: {params}")
    try:
        start_time = time.time()
        result = subprocess.run(
            [sys.executable, script_path],
            capture_output=True,
            text=True,
            env=env,
            timeout=600
        )
        elapsed = time.time() - start_time
        logger.info(f"Script {script_path} finished in {elapsed:.2f}s with return code {result.returncode}.")
        if result.stdout:
            logger.info(f"Script output: {result.stdout[:4000]}")
        if result.stderr:
            logger.warning(f"Script error: {result.stderr[:2000]}")
        if result.returncode == 0:
            return True, result.stdout, ""
        else:
            return False, result.stdout, result.stderr
    except Exception as e:
        logger.error(f"Exception running script {script_path}: {e}", exc_info=True)
        return False, "", str(e)

def build_execution_order(modules):
    """Always sorts modules by strict pipeline order."""
    order = [m['id'] for t in type_order for m in modules if m['type'] == t]
    # Add any not in type_order at the end
    order += [m['id'] for m in modules if m['id'] not in order]
    logger.info(f"Execution order determined: {order}")
    return order

def get_script_path(module_type):
    mapping = {
        "scrape": os.path.join(SCRIPTS_PATH, "scraping", "scraping2.py"),
        "parse_pdf": os.path.join(SCRIPTS_PATH, "ingest", "parsers", "parse_pdf.py"),
        "parse_csv": os.path.join(SCRIPTS_PATH, "ingest", "parsers", "parse_csv.py"),
        "parse_excel": os.path.join(SCRIPTS_PATH, "ingest", "parsers", "parse_excel.py"),
        "parse_json": os.path.join(SCRIPTS_PATH, "ingest", "parsers", "parse_json.py"),
        "parse_docx": os.path.join(SCRIPTS_PATH, "ingest", "parsers", "parse_docx.py"),
        "parse_text": os.path.join(SCRIPTS_PATH, "ingest", "parsers", "parse_text.py"),
        "chunk_texts": os.path.join(SCRIPTS_PATH, "rag2", "chunking.py"),
        "embed_store": os.path.join(SCRIPTS_PATH, "rag2", "embedding.py"),
        "rag_query": os.path.join(SCRIPTS_PATH, "rag2", "rag_query.py"),
        # Extend for others if needed
    }
    path = mapping.get(module_type)
    logger.info(f"Resolved script path for '{module_type}': {path}")
    return path

def pipeline_worker(pipeline_id, modules, connections):
    logger.info(f"Pipeline {pipeline_id} started with modules: {modules}")
    pipeline_status = {
        "status": "running",
        "modules": {},
        "results": {}
    }

    try:
        cleanup_outputs()
        execution_order = build_execution_order(modules)
        id_to_module = {m['id']: m for m in modules}

        for module_id in execution_order:
            module = id_to_module[module_id]
            mod_type = module['type']
            
            # Check if this is a RAG query module with empty question
            if mod_type == 'rag_query':
                params = module.get("parameters", {})
                question = params.get('question', '').strip()
                
                if not question:
                    # Set module status to waiting for input
                    pipeline_status["modules"][module_id] = {
                        "status": "waiting_for_input",
                        "message": "Waiting for user question"
                    }
                    pipeline_status["status"] = "waiting_for_input"
                    pipeline_status["waiting_module"] = module_id
                    ACTIVE_PIPELINES[pipeline_id] = pipeline_status.copy()
                    
                    # Wait for question to be provided
                    while (pipeline_id in ACTIVE_PIPELINES and 
                           ACTIVE_PIPELINES[pipeline_id]["status"] == "waiting_for_input"):
                        time.sleep(0.5)
                    
                    # CRITICAL FIX: Update the module parameters permanently
                    if pipeline_id in ACTIVE_PIPELINES:
                        updated_params = ACTIVE_PIPELINES[pipeline_id].get("updated_params", {})
                        if updated_params and "question" in updated_params:
                            # Update both the local params and the module itself
                            params["question"] = updated_params["question"]
                            module["parameters"]["question"] = updated_params["question"]
                            # Update the id_to_module mapping as well
                            id_to_module[module_id]["parameters"]["question"] = updated_params["question"]
                            
                            logger.info(f"Updated question for module {module_id}: {updated_params['question']}")

            script_path = get_script_path(mod_type)
            if not script_path or not os.path.isfile(script_path):
                msg = f"Script not found for {mod_type} ({script_path})"
                logger.error(msg)
                pipeline_status["modules"][module_id] = {
                    "status": "failed",
                    "message": msg
                }
                pipeline_status["status"] = "failed"
                break

            # Use the updated params for script execution
            params = module.get("parameters", {})
            pipeline_status["modules"][module_id] = {"status": "running", "message": "Started"}
            ACTIVE_PIPELINES[pipeline_id] = pipeline_status.copy()

            logger.info(f"Executing module '{mod_type}' (ID: {module_id}) with params: {params}")
            success, out, err = run_script(script_path, params)

            if success:
                pipeline_status["modules"][module_id] = {
                    "status": "completed",
                    "message": out[-1000:]
                }
                pipeline_status["results"][module_id] = {
                    "output": out[-4000:]
                }
                logger.info(f"Module {module_id} ({mod_type}) completed.")
            else:
                logger.error(f"Module {module_id} ({mod_type}) failed. Error: {err}")
                pipeline_status["modules"][module_id] = {
                    "status": "failed",
                    "message": err[-2000:]
                }
                pipeline_status["status"] = "failed"
                ACTIVE_PIPELINES[pipeline_id] = pipeline_status.copy()
                break
        else:
            pipeline_status["status"] = "completed"
            logger.info(f"Pipeline {pipeline_id} completed successfully.")
            ACTIVE_PIPELINES[pipeline_id] = pipeline_status.copy()

    except Exception as e:
        logger.error(f"Exception in pipeline {pipeline_id}: {e}", exc_info=True)
        pipeline_status["status"] = "failed"
        pipeline_status["error"] = str(e)
        ACTIVE_PIPELINES[pipeline_id] = pipeline_status.copy()

@app.route("/api/pipeline/execute", methods=["POST"])
def api_pipeline_execute():
    data = request.json
    modules = data.get("modules", [])
    connections = data.get("connections", [])
    pipeline_id = str(uuid.uuid4())
    logger.info(f"Received new pipeline execution request. ID: {pipeline_id}")
    ACTIVE_PIPELINES[pipeline_id] = {"status": "pending", "modules": {}, "results": {}}
    thread = threading.Thread(
        target=pipeline_worker, args=(pipeline_id, modules, connections), name=f"Pipeline-{pipeline_id[:8]}"
    )
    thread.daemon = True
    thread.start()
    return jsonify({"pipeline_id": pipeline_id, "status": "running"})

@app.route("/api/pipeline/update_question/<pipeline_id>", methods=["POST"])
def api_update_question(pipeline_id):
    data = request.json
    question = data.get("question", "")
    
    if pipeline_id not in ACTIVE_PIPELINES:
        return jsonify({"error": "Pipeline not found"}), 404
    
    pipeline_status = ACTIVE_PIPELINES[pipeline_id]
    if pipeline_status.get("status") != "waiting_for_input":
        return jsonify({"error": "Pipeline not waiting for input"}), 400
    
    # Update the pipeline with the question
    pipeline_status["updated_params"] = {"question": question}
    pipeline_status["status"] = "running"
    
    # Clean up waiting state
    pipeline_status.pop("waiting_module", None)
    
    ACTIVE_PIPELINES[pipeline_id] = pipeline_status
    
    logger.info(f"Question updated for pipeline {pipeline_id}: {question}")
    return jsonify({"status": "success"})

@app.route("/api/pipeline/status/<pipeline_id>")
def api_pipeline_status(pipeline_id):
    status = ACTIVE_PIPELINES.get(pipeline_id, None)
    if status is None:
        logger.warning(f"Status check for non-existent pipeline: {pipeline_id}")
        return jsonify({"error": "Pipeline not found"}), 404
    logger.info(f"Status check for pipeline {pipeline_id}: {status.get('status')}")
    return jsonify(status)

@app.route("/api/pipeline/results/<pipeline_id>")
def api_pipeline_results(pipeline_id):
    status = ACTIVE_PIPELINES.get(pipeline_id, None)
    if status is None:
        logger.warning(f"Results check for non-existent pipeline: {pipeline_id}")
        return jsonify({"error": "Pipeline not found"}), 404
    logger.info(f"Results requested for pipeline {pipeline_id}")
    return jsonify({"results": status.get("results", {})})

if __name__ == "__main__":
    logger.info("Starting Flask app on 0.0.0.0:5000")
    app.run(host="0.0.0.0", port=5000, debug=False)
